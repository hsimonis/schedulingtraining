\documentclass[a4paper]{report}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{hyperref}
\title{Results for Scheduling Benchmark Classes}
\author{Luis Quesada and Helmut Simonis}
\begin{document}
\maketitle
\begin{abstract}
this reposts lists results of the \emph{tbischeduling} tool for a number of existing benchmarks on scheduling related problems. THe results indicate that depending on the problem type, only a fraction of the benchmarks are solved to optimality, while good or reasonable results are obtained by CPOptimizer of IBM.
\end{abstract}

\tableofcontents

\chapter{Introduction}

The results are obtained by running the TestAll main routine for the different benchmark problems, selecting the necessary parameters and limits for each benchmark type.

The detailed execution time depends on many parameters that are not well controlled in the test environment, so the results should be considered with caution. Tests were run on a Windows 11 laptop using CPOptimizer 22.1.0, and CPSat 9.11, both using their Java API. 

\chapter{Overview}

The following tables compare the results of CPOptimizer and CPSat on a number of well-known benchmark problems. Note that the programs used are the generic solutions of the problems, there was no attempt to improve lower bounds, or add redundant constraints that would improve pruning for a specific problem class. As such, the results indicate "out-of-the-box"  performance of the solvers.

The comparison does not attempt to compare the solutions found to the best known results, it is only intended to compare the results of the solvers considered based on the same underlying data model, running on the same hardware, with the same time limit.


Table~\ref{tab:compareoss} shows the results for the Taillard open shop problem set. All problems are solved to optimality by both solvers, given 600 seconds timeout and 4 resp. 8 threads for the solver. The results are grouped by instances classes where $x/y$ indicate $x$ jobs on $y$ machines. We compare the time taken to find and prove the optimal solution based on the virtual best solution of the faster time of the solvers. For smaller instances, CPSat seems to be faster, but for larger instances CPOptimizer finds the solution more rapidly. Given that all results are obtained in atmost a few seconds on either solver, this does not seem to be a significant difference.

\input{compareoss}

\input{comparejss}

Table~\ref{tab:comparejss} compares the results for the Taillard job shop problems. Only some of the problem groups are solved to optimality. For the 10/20 set, CPOptimizer proves optimality, while CPSat finds solutions which are very close to the optimal results. The bound results for 100/20 with CPSat are incorrect, and need to be recomputed.

\input{comparefss}


Table~\ref{tab:comparefss} shows the results for the Taillard flow shop problems. Instance sets 20/5 and 50/5 are solved to optimality by both solvers, CPOptimizer finds many optimal solutions for the 100/5 sets. There are a few optimal solutions for the 20/10 set as well. Execution times for the optimal solutions seems to be significantly higher for CPSat. Comparing the non-optimal solutions, CPOptimizer consistently finds better solutions, but on average, CPSat is within 10\% of the CPOptimizer result. For the bounds, CPSat often provide slightly better lower bounds, but CPOptimizer results are pretty close. Note that for the bounds, a higher value is better, so achieving 100\% of the virtual best bound is better than achieving 98\%.

Table~\ref{tab:comparepfss} compares the results for CPOptimizer for the regular flow shop and the permutation flow shop version on the same data. The model for the permutation flow shop is not available with CPSat. A number of instances are solved to optimality with the permutation flow shop variant, but these optimal solutions typically are not optimal for the unrestricted version. In general, it is much faster to find the optimal solution for the permutation flowshop version, and perhaps surprisingly, the results for the non-optimal instances are often superior for the permutation flowshop. The bounds for the permutation flowshop are stronger, but they are not valid bounds for the unrestricted version. 

\input{comparepfss}


\input{comparesalbp}

\input{comparesalbpalternative}

The results for SALBP-1 in Table~\ref{tab:comparesalbp} were obtained with a 30 second timeout. All of the 20 task instances were solved to optimality (some only with CPSat), while 88\% of the 50 task instances were solved to optimality, but only 43\% were solved by both. The time taken to find the common optimal solutions varies significantly, CPSat seems on average faster on the small instances, while CPOptimizer is faster on a larger instances, but this does not hold for all instances. For the non-optimal solutions, solution quality seems very evenly balanced.

The results for the test scheduling case study are shown in Table~\ref{tab:compareTest}.
Very likely the 30 second timeout is too small for the large problem instances. Optimal solutions are found by both solvers for the smaller instance sizes, CPO does find some optimal solutions even for large problem sizes. Solution quality for up to 100 tasks is very close, while results for CPSat on the 500 task problems is disappointing compared to the CPO results. Already for 100 tasks, the time needed to find the optimal solutions is much higher for CPSat, the results may improve if more time is given for both solvers.

\input{compareTest}

Table~\ref{tab:compareTrans} compares CPO and CPSat for the Hybrid flexible flow shop problem of the factory design case study. For the smaller problem sizes, up to 40 jobs, both system offer comparable solution quality, with significantly high run times for CPSat. Starting from the 50 job problem instances, but especially for the large 300 and 400 job problems, the solution quality of CPO is much better. Optimal solutions are found by both system up to size 30, CPSat finds more optimal solutions for the smaller problems, and CPO finds more optimal solution for larger instances (up to size 50). 

While the results for CPO are clearly superior to the ones for CPSat for the 500 task problem instances, the given lower bound is very poor for CPO. 

\input{compareTrans}

Note that there is a single 300 job instance 300\_2 for which CPSat did not find any solution within the timeout, so only 24 instances are compared.

The results for the single mode RCPSP problems do not use consistent settings, the smaller (j30 and j60) instances are run with a 600 seconds timeout, the j90 instances with a 30 seconds, and the j120 instances with a 60 second timeout. These larger instances should be rerun with a 600 seconds timeout, but that will require several days of CPU time.  

%\input{comparercpspj30} 
%\input{comparercpspj60} 
%\input{comparercpspj90} 
\input{comparercpspj120} 

Overall, the results are similar to other problem sets. For all instance sets, both CPO and SPSat find and prove optimal solutions, 100\% for J30 instances, 90\% for j60 instances, 80\% for j90 instances, but only 45\% for j120 instances. While the times for the j30 instances are comparable, the overall time for the larger instances is significantly higher for CPSat. Detailed results shows that many instances are solved by both solvers in less than a second, the differences arise from relatively few instances that take much longer in CPSat, while there are only few instances where CPO takes longer than CPSat.

For all non-optimal solutions, the solution quality obtained by both solvers within the timeout is nearly identical, with a very slight advantage for CPO for both solutions found and lower bound calculated.

\clearpage
\chapter{Taillard Open Shop Problems}
All problems are solved to optimality, possibly due to their small to moderate size.

\section{Results for CPOptimizer}

\input{resultsoss}

\section{Results for CPSat}

\input{resultsossCPSat}

\clearpage
\chapter{Taillard Job Shop Problems}

The results are rather confusing, as some smaller problems cannot be solved to optimality, while complete groups of larger instances can. The number of jobs clearly is not the only indicator of difficulty of these problems.

\section{Results for CPOptimizer}

\input{resultsjss}

\section{Results for CPSat}

\input{resultsjssCPSat}

\section{Sample Results on Mac (CPOptimizer)}
For a selected subset of the tests, we also tried running on a mac laptop, results show some good improvement of the m2 based laptop over the Intel based Windows machine, but the improvements are not consistent.
\input{resultsjss100-20-mac}

\clearpage
\chapter{Taillard Flow Shop Problems}

These problems seem to be more difficult to solve to optimality. The number of stages seems to make a huge difference, we can solve the problems with five stages (machines) much more easily than the problems with 10 or twenty stages.

\section{Results for CPOptimizer}

\input{resultsfss}

\section{Results for CPSat}

\input{resultsfssCPSat}

\section{Permutation Flowshop Results for CPOptimizer}

We can run the flowshop benchmarks with an additional constraint to be solved as a permutation flowshop, which dramatically reduces the sets of feasible solutions, and the search tree to be searched. This might results in improved solutions found as a larger part of that search space can be explored, but solutions can be worse than for the original problem. In particular the optimal solution for the permutation flowshop can be worse than a good feasible solution for the unrestricted flowshop.

\input{resultspfss}

\clearpage
\chapter{SALBP-1 Assembly Line Balancing Problems}

The assembly line balancing problems have a single cumulative and no disjunctive constraints, so the indicated number of (disjunctive) machines is zero. 

The larger problem instances are still missing. For the small instances (20 tasks), only a few are not solved to optimality, for the medium sizes the number of optimal solutions found is reduced, and for larger instances, optimal solutions are rare.

\section{Results for CPOptimizer}

\input{resultssalbp}

\section{Results for CPSat}

\input{resultssalbpCPSat}

\section{Results for MiniZinc/Cplex}

\input{resultssalbpCplex}

\section{Results for MiniZinc/Chuffed}

\input{resultssalbpChuffed}

\section{Results for MiniZinc/CPSat}

\input{resultssalbpMiniCPSat}

\section{Alternative Model}

\subsection{CPO}

\input{resultsalternativesalbp}

\subsection{CPSat}

\input{resultsalternativesalbpCPSat}

\subsection{Chuffed}

\input{resultsalternativesalbpChuffed}

\subsection{Cplex}

\input{resultsalternativesalbpCplex}


\clearpage
\chapter{Test Scheduling Problems}

Due to the number of instances given, we only run problems for 30 seconds, some results are still missing. The original instance data was given in Prolog format, we generate a JSON equivalent, which is used as input to create the problems.

\section{Results for CPOptimizer}

\input{resultstsched}

\section{Results for CPSat}

\input{resultstschedCPSat}

\clearpage
\chapter{J\&J Hybrid Flexible Flowshop with Transportation Times}

\section{Without Transportation Times}

\subsection{Results for CPOptimizer}

\input{resultstrans}

\subsection{Results for CPSat}

\input{resultstransCPSat}

\clearpage
\chapter{RCPSP SingleMode}

The detailed result tables for the individual RCPSP instances show that many instances are solved with either solver in less than a second, but that there are a few families of problems which are more difficult to solve. For problem type J30, the sets 13, 29, and 45 are such examples, they are still solved to optimality, but the time required is larger. For bigger problem instances, the solvers are not able to find and prove the optimal solutions for such families, for example sets 9, 13 , 25, 29, 41, 45 for j60. It would be interesting to understand this better, and see which generator settings make these instance families more difficult to solve.

\section{Size J30}
\subsection{CPO}
\input{resultsrcpspj30CPO}

\subsection{CPSat}
\input{resultsrcpspj30CPSat}

\section{Size J60}
\subsection{CPO}
\input{resultsrcpspj60CPO}

\subsection{CPSat}
\input{resultsrcpspj60CPSat}

\section{Size J90}
\subsection{CPO}
\input{resultsrcpspj90CPO}

\subsection{CPSat}
\input{resultsrcpspj90CPSat}

\section{Size J120}
\subsection{CPO}
\input{resultsrcpspj120CPO}

\subsection{CPSat}
\input{resultsrcpspj120CPSat}

\clearpage
\chapter{Result Comparison for SALBP}

\input{salbpresultsummary}
\clearpage

\input{salbpresultcompare}

\end{document}